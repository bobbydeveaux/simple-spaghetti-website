name: ML Model Training and Deployment

on:
  schedule:
    # Run every Monday at 2 AM UTC after race weekends
    - cron: '0 2 * * 1'

  workflow_dispatch:
    inputs:
      trigger_reason:
        description: 'Reason for manual trigger'
        required: true
        default: 'manual-retrain'
        type: choice
        options:
        - manual-retrain
        - new-race-data
        - model-improvement
        - emergency-update
      season:
        description: 'Season to include in training (leave empty for all)'
        required: false
        type: string
      force_retrain:
        description: 'Force full retrain even if no new data'
        required: false
        type: boolean
        default: false

  push:
    branches: [ main ]
    paths:
      - 'f1-analytics/backend/ml/**'
      - 'f1-analytics/backend/data_ingestion/**'

env:
  PYTHON_VERSION: '3.11'
  S3_BUCKET_MODELS: 'f1-analytics-models'

jobs:
  # Check if new race data is available
  check-data-freshness:
    name: Check Data Freshness
    runs-on: ubuntu-latest

    outputs:
      new_races_available: ${{ steps.check.outputs.new_races }}
      last_model_date: ${{ steps.check.outputs.last_model_date }}
      latest_race_date: ${{ steps.check.outputs.latest_race_date }}
      should_retrain: ${{ steps.check.outputs.should_retrain }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        pip install psycopg2-binary pandas boto3

    - name: Check for new race data
      id: check
      env:
        DATABASE_URL: ${{ secrets.PRODUCTION_DATABASE_URL }}
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      run: |
        python << 'EOF'
        import os
        import psycopg2
        import boto3
        from datetime import datetime, timedelta

        # Connect to database
        conn = psycopg2.connect(os.environ['DATABASE_URL'])
        cur = conn.cursor()

        # Check latest race completion
        cur.execute("""
            SELECT MAX(race_date), COUNT(*)
            FROM races
            WHERE status = 'completed'
            AND race_date > NOW() - INTERVAL '30 days'
        """)
        latest_race, recent_races = cur.fetchone()

        # Check last model training date from S3 metadata
        s3 = boto3.client('s3')
        try:
            response = s3.list_objects_v2(
                Bucket=os.environ.get('S3_BUCKET_MODELS', 'f1-analytics-models'),
                Prefix='production/ensemble_',
                MaxKeys=1
            )

            if 'Contents' in response:
                last_model_date = response['Contents'][0]['LastModified']
            else:
                last_model_date = datetime.min

        except Exception:
            last_model_date = datetime.min

        # Determine if retraining is needed
        should_retrain = False
        reasons = []

        if recent_races > 0:
            reasons.append(f"{recent_races} recent race(s) completed")
            should_retrain = True

        if latest_race and last_model_date:
            days_since_update = (datetime.now() - last_model_date.replace(tzinfo=None)).days
            if days_since_update > 14:
                reasons.append(f"Model {days_since_update} days old")
                should_retrain = True

        # Force retrain if manually requested
        if os.environ.get('GITHUB_EVENT_NAME') == 'workflow_dispatch':
            force_retrain = os.environ.get('INPUT_FORCE_RETRAIN', 'false').lower() == 'true'
            if force_retrain:
                reasons.append("Manual force retrain requested")
                should_retrain = True

        # Output results
        with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
            f.write(f"new_races={recent_races}\n")
            f.write(f"last_model_date={last_model_date}\n")
            f.write(f"latest_race_date={latest_race or 'None'}\n")
            f.write(f"should_retrain={str(should_retrain).lower()}\n")

        print(f"Retraining needed: {should_retrain}")
        print(f"Reasons: {', '.join(reasons) if reasons else 'None'}")

        conn.close()
        EOF

  # Data ingestion and validation
  data-ingestion:
    name: Ingest Latest Race Data
    runs-on: ubuntu-latest
    needs: check-data-freshness
    if: needs.check-data-freshness.outputs.should_retrain == 'true'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install httpx asyncio

    - name: Run data ingestion
      env:
        DATABASE_URL: ${{ secrets.PRODUCTION_DATABASE_URL }}
        ERGAST_API_KEY: ${{ secrets.ERGAST_API_KEY }}
        WEATHER_API_KEY: ${{ secrets.WEATHER_API_KEY }}
      run: |
        # Run F1 data ingestion if the service exists
        if [ -d "f1-analytics/backend/data_ingestion" ]; then
          cd f1-analytics/backend

          # Ingest latest race results
          python -m data_ingestion.ingest_races --season 2026 --latest-only

          # Ingest qualifying results
          python -m data_ingestion.ingest_qualifying --season 2026 --latest-only

          # Ingest weather data
          python -m data_ingestion.ingest_weather --season 2026 --latest-only

          echo "‚úÖ Data ingestion completed"
        else
          echo "F1 analytics data ingestion not yet implemented"
        fi

    - name: Validate ingested data
      env:
        DATABASE_URL: ${{ secrets.PRODUCTION_DATABASE_URL }}
      run: |
        python << 'EOF'
        import psycopg2
        import os

        conn = psycopg2.connect(os.environ['DATABASE_URL'])
        cur = conn.cursor()

        # Check data quality
        checks = [
            ("races", "SELECT COUNT(*) FROM races WHERE race_date > NOW() - INTERVAL '7 days'"),
            ("race_results", "SELECT COUNT(*) FROM race_results rr JOIN races r ON rr.race_id = r.race_id WHERE r.race_date > NOW() - INTERVAL '7 days'"),
            ("weather_data", "SELECT COUNT(*) FROM weather_data wd JOIN races r ON wd.race_id = r.race_id WHERE r.race_date > NOW() - INTERVAL '7 days'")
        ]

        for table, query in checks:
            cur.execute(query)
            count = cur.fetchone()[0]
            print(f"{table}: {count} recent records")

            if count == 0 and table != "weather_data":  # Weather data might not be available immediately
                print(f"‚ö†Ô∏è Warning: No recent {table} found")

        conn.close()
        print("‚úÖ Data validation completed")
        EOF

  # Feature engineering and model training
  train-models:
    name: Train ML Models
    runs-on: ubuntu-latest
    needs: [check-data-freshness, data-ingestion]
    if: needs.check-data-freshness.outputs.should_retrain == 'true'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python with ML libraries
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install ML dependencies
      run: |
        pip install -r requirements.txt
        pip install scikit-learn==1.3.2 xgboost==2.0.3 pandas numpy joblib matplotlib seaborn

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ secrets.AWS_REGION }}

    - name: Feature engineering
      env:
        DATABASE_URL: ${{ secrets.PRODUCTION_DATABASE_URL }}
      run: |
        if [ -d "f1-analytics/backend/ml" ]; then
          cd f1-analytics/backend

          echo "üîß Starting feature engineering..."
          python << 'EOF'
from ml.feature_engineering import FeatureEngineering
from database import SessionLocal
import pandas as pd

# Initialize database session
db = SessionLocal()
feature_eng = FeatureEngineering(db)

# Extract features for all completed races
print("Extracting features from historical data...")
features_df = feature_eng.build_training_dataset(start_season=2010)

print(f"Feature matrix shape: {features_df.shape}")
print(f"Features: {list(features_df.columns)}")

# Save features for training
features_df.to_csv('training_features.csv', index=False)
print("‚úÖ Feature engineering completed")

db.close()
EOF
        else
          echo "‚ö†Ô∏è ML module not yet implemented, using mock training"

          # Create mock feature data for testing
          python << 'EOF'
import pandas as pd
import numpy as np

# Create mock training data
n_races = 100
n_drivers_per_race = 20

features = []
for race in range(n_races):
    for driver in range(n_drivers_per_race):
        features.append({
            'race_id': race,
            'driver_id': driver,
            'elo_rating': np.random.normal(1500, 200),
            'recent_form': np.random.uniform(5, 15),
            'track_performance': np.random.uniform(0, 1),
            'weather_factor': np.random.uniform(0.8, 1.2),
            'is_winner': 1 if driver == 0 else 0  # Driver 0 wins each race
        })

df = pd.DataFrame(features)
df.to_csv('training_features.csv', index=False)
print("‚úÖ Mock feature data created")
EOF
        fi

    - name: Train Random Forest model
      run: |
        python << 'EOF'
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import brier_score_loss, log_loss, accuracy_score
import joblib
import os

# Load features
print("Loading training data...")
df = pd.read_csv('training_features.csv')

# Prepare features and target
feature_cols = [col for col in df.columns if col not in ['race_id', 'driver_id', 'is_winner']]
X = df[feature_cols]
y = df['is_winner']

print(f"Training on {len(X)} samples with {len(feature_cols)} features")

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Train Random Forest
print("Training Random Forest model...")
rf_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    min_samples_split=5,
    class_weight='balanced',
    random_state=42,
    n_jobs=-1
)

rf_model.fit(X_train, y_train)

# Evaluate model
y_pred_proba = rf_model.predict_proba(X_test)[:, 1]
y_pred = rf_model.predict(X_test)

brier_score = brier_score_loss(y_test, y_pred_proba)
log_loss_score = log_loss(y_test, y_pred_proba)
accuracy = accuracy_score(y_test, y_pred)

print(f"Random Forest Performance:")
print(f"  Brier Score: {brier_score:.4f}")
print(f"  Log Loss: {log_loss_score:.4f}")
print(f"  Accuracy: {accuracy:.4f}")

# Save model
os.makedirs('models', exist_ok=True)
joblib.dump(rf_model, 'models/random_forest.pkl')

# Save metrics
with open('rf_metrics.txt', 'w') as f:
    f.write(f"brier_score={brier_score}\n")
    f.write(f"log_loss={log_loss_score}\n")
    f.write(f"accuracy={accuracy}\n")

print("‚úÖ Random Forest training completed")
EOF

    - name: Train XGBoost model
      run: |
        python << 'EOF'
import pandas as pd
import numpy as np
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import brier_score_loss, log_loss, accuracy_score
import joblib
import os

# Load features
df = pd.read_csv('training_features.csv')

feature_cols = [col for col in df.columns if col not in ['race_id', 'driver_id', 'is_winner']]
X = df[feature_cols]
y = df['is_winner']

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Calculate class weights for imbalanced data
pos_weight = (y == 0).sum() / (y == 1).sum()

# Train XGBoost
print("Training XGBoost model...")
xgb_model = XGBClassifier(
    learning_rate=0.1,
    n_estimators=200,
    max_depth=6,
    scale_pos_weight=pos_weight,
    random_state=42,
    eval_metric='logloss'
)

xgb_model.fit(X_train, y_train)

# Evaluate model
y_pred_proba = xgb_model.predict_proba(X_test)[:, 1]
y_pred = xgb_model.predict(X_test)

brier_score = brier_score_loss(y_test, y_pred_proba)
log_loss_score = log_loss(y_test, y_pred_proba)
accuracy = accuracy_score(y_test, y_pred)

print(f"XGBoost Performance:")
print(f"  Brier Score: {brier_score:.4f}")
print(f"  Log Loss: {log_loss_score:.4f}")
print(f"  Accuracy: {accuracy:.4f}")

# Save model
joblib.dump(xgb_model, 'models/xgboost.pkl')

# Save metrics
with open('xgb_metrics.txt', 'w') as f:
    f.write(f"brier_score={brier_score}\n")
    f.write(f"log_loss={log_loss_score}\n")
    f.write(f"accuracy={accuracy}\n")

print("‚úÖ XGBoost training completed")
EOF

    - name: Create ensemble model
      run: |
        python << 'EOF'
import pandas as pd
import numpy as np
import joblib
from sklearn.model_selection import train_test_split
from sklearn.metrics import brier_score_loss
import os

# Load models
rf_model = joblib.load('models/random_forest.pkl')
xgb_model = joblib.load('models/xgboost.pkl')

# Load test data
df = pd.read_csv('training_features.csv')
feature_cols = [col for col in df.columns if col not in ['race_id', 'driver_id', 'is_winner']]
X = df[feature_cols]
y = df['is_winner']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Create ensemble predictions
rf_proba = rf_model.predict_proba(X_test)[:, 1]
xgb_proba = xgb_model.predict_proba(X_test)[:, 1]

# Simple average ensemble
ensemble_proba = (rf_proba + xgb_proba) / 2

# Evaluate ensemble
ensemble_brier = brier_score_loss(y_test, ensemble_proba)

print(f"Ensemble Performance:")
print(f"  Brier Score: {ensemble_brier:.4f}")

# Save ensemble metadata
with open('ensemble_metrics.txt', 'w') as f:
    f.write(f"brier_score={ensemble_brier}\n")
    f.write(f"rf_weight=0.5\n")
    f.write(f"xgb_weight=0.5\n")

print("‚úÖ Ensemble model created")
EOF

    - name: Upload models to S3
      run: |
        # Create versioned model artifacts
        MODEL_VERSION=$(date +%Y%m%d-%H%M%S)-${GITHUB_SHA:0:7}

        # Upload individual models
        aws s3 cp models/random_forest.pkl s3://${{ env.S3_BUCKET_MODELS }}/versions/random_forest_${MODEL_VERSION}.pkl
        aws s3 cp models/xgboost.pkl s3://${{ env.S3_BUCKET_MODELS }}/versions/xgboost_${MODEL_VERSION}.pkl

        # Upload to staging for testing
        aws s3 cp models/random_forest.pkl s3://${{ env.S3_BUCKET_MODELS }}/staging/random_forest.pkl
        aws s3 cp models/xgboost.pkl s3://${{ env.S3_BUCKET_MODELS }}/staging/xgboost.pkl

        # Upload metrics
        aws s3 cp rf_metrics.txt s3://${{ env.S3_BUCKET_MODELS }}/metrics/rf_metrics_${MODEL_VERSION}.txt
        aws s3 cp xgb_metrics.txt s3://${{ env.S3_BUCKET_MODELS }}/metrics/xgb_metrics_${MODEL_VERSION}.txt
        aws s3 cp ensemble_metrics.txt s3://${{ env.S3_BUCKET_MODELS }}/metrics/ensemble_metrics_${MODEL_VERSION}.txt

        # Create model manifest
        cat > model_manifest.json << EOF
{
  "version": "${MODEL_VERSION}",
  "created_at": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
  "commit_sha": "${GITHUB_SHA}",
  "models": {
    "random_forest": "s3://${{ env.S3_BUCKET_MODELS }}/versions/random_forest_${MODEL_VERSION}.pkl",
    "xgboost": "s3://${{ env.S3_BUCKET_MODELS }}/versions/xgboost_${MODEL_VERSION}.pkl"
  },
  "metrics": {
    "random_forest": $(cat rf_metrics.txt | sed 's/=/":"/g; s/^/"/; s/$/"/' | tr '\n' ',' | sed 's/,$//' | sed 's/^/{/' | sed 's/$/}/'),
    "xgboost": $(cat xgb_metrics.txt | sed 's/=/":"/g; s/^/"/; s/$/"/' | tr '\n' ',' | sed 's/,$//' | sed 's/^/{/' | sed 's/$/}/'),
    "ensemble": $(cat ensemble_metrics.txt | sed 's/=/":"/g; s/^/"/; s/$/"/' | tr '\n' ',' | sed 's/,$//' | sed 's/^/{/' | sed 's/$/')
  }
}
EOF

        aws s3 cp model_manifest.json s3://${{ env.S3_BUCKET_MODELS }}/manifests/manifest_${MODEL_VERSION}.json

        echo "MODEL_VERSION=$MODEL_VERSION" >> $GITHUB_ENV

    - name: Upload training artifacts
      uses: actions/upload-artifact@v3
      with:
        name: ml-models-${{ env.MODEL_VERSION }}
        path: |
          models/
          *_metrics.txt
          model_manifest.json
        retention-days: 30

  # Test models in staging
  test-staging:
    name: Test Models in Staging
    runs-on: ubuntu-latest
    needs: train-models
    environment:
      name: staging

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        pip install -r requirements.txt

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ secrets.AWS_REGION }}

    - name: Test model loading and inference
      env:
        DATABASE_URL: ${{ secrets.STAGING_DATABASE_URL }}
      run: |
        python << 'EOF'
import boto3
import joblib
import pandas as pd
import numpy as np
import io

# Download models from S3 staging
s3 = boto3.client('s3')

try:
    # Download Random Forest model
    rf_obj = s3.get_object(Bucket='${{ env.S3_BUCKET_MODELS }}', Key='staging/random_forest.pkl')
    rf_model = joblib.load(io.BytesIO(rf_obj['Body'].read()))

    # Download XGBoost model
    xgb_obj = s3.get_object(Bucket='${{ env.S3_BUCKET_MODELS }}', Key='staging/xgboost.pkl')
    xgb_model = joblib.load(io.BytesIO(xgb_obj['Body'].read()))

    print("‚úÖ Models loaded successfully from S3")

    # Create test data for inference
    test_features = pd.DataFrame({
        'elo_rating': [1600, 1550, 1500, 1450],
        'recent_form': [8.5, 10.2, 12.1, 14.8],
        'track_performance': [0.8, 0.6, 0.4, 0.2],
        'weather_factor': [1.0, 1.0, 1.0, 1.0]
    })

    # Test inference
    rf_predictions = rf_model.predict_proba(test_features)[:, 1]
    xgb_predictions = xgb_model.predict_proba(test_features)[:, 1]

    # Ensemble prediction
    ensemble_predictions = (rf_predictions + xgb_predictions) / 2

    print(f"‚úÖ Inference test successful")
    print(f"Sample predictions: {ensemble_predictions}")

    # Verify predictions sum to reasonable values (not exactly 100% as this is per-driver)
    print(f"Prediction range: {ensemble_predictions.min():.3f} - {ensemble_predictions.max():.3f}")

except Exception as e:
    print(f"‚ùå Model testing failed: {e}")
    exit(1)
EOF

    - name: Update staging prediction service
      env:
        STAGING_API_URL: ${{ secrets.STAGING_API_URL }}
      run: |
        # Trigger cache refresh in staging environment
        curl -X POST "${STAGING_API_URL}/api/v1/admin/models/reload" \
          -H "Authorization: Bearer ${{ secrets.ADMIN_JWT_TOKEN }}" \
          -H "Content-Type: application/json" \
          -d '{"source": "staging"}' || echo "Model reload endpoint not available yet"

    - name: Run prediction accuracy tests
      run: |
        # Test prediction generation for upcoming races
        python << 'EOF'
import requests
import os

api_url = os.environ.get('STAGING_API_URL', 'http://localhost:8000')

try:
    # Test prediction endpoint
    response = requests.get(
        f"{api_url}/api/v1/predictions/next-race",
        headers={"Authorization": f"Bearer {os.environ.get('ADMIN_JWT_TOKEN', 'test-token')}"}
    )

    if response.status_code == 200:
        predictions = response.json()
        print(f"‚úÖ Predictions endpoint working")
        print(f"Found {len(predictions.get('predictions', []))} driver predictions")
    else:
        print(f"‚ö†Ô∏è Predictions endpoint returned {response.status_code}")

except requests.exceptions.RequestException as e:
    print(f"‚ö†Ô∏è Could not test staging API: {e}")
    print("This is expected if staging environment is not fully deployed")
EOF

  # Deploy to production
  deploy-production:
    name: Deploy Models to Production
    runs-on: ubuntu-latest
    needs: [train-models, test-staging]
    if: needs.test-staging.result == 'success'
    environment:
      name: production

    steps:
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ secrets.AWS_REGION }}

    - name: Promote models from staging to production
      run: |
        # Copy staging models to production
        aws s3 cp s3://${{ env.S3_BUCKET_MODELS }}/staging/random_forest.pkl s3://${{ env.S3_BUCKET_MODELS }}/production/random_forest.pkl
        aws s3 cp s3://${{ env.S3_BUCKET_MODELS }}/staging/xgboost.pkl s3://${{ env.S3_BUCKET_MODELS }}/production/xgboost.pkl

        # Update production model metadata
        TIMESTAMP=$(date -u +%Y-%m-%dT%H:%M:%SZ)
        cat > production_model_meta.json << EOF
{
  "deployed_at": "$TIMESTAMP",
  "commit_sha": "${{ github.sha }}",
  "previous_version": "$(aws s3api head-object --bucket ${{ env.S3_BUCKET_MODELS }} --key production/model_metadata.json --query LastModified --output text 2>/dev/null || echo 'None')",
  "deployment_method": "automated",
  "trigger": "${{ github.event_name }}"
}
EOF

        aws s3 cp production_model_meta.json s3://${{ env.S3_BUCKET_MODELS }}/production/model_metadata.json

    - name: Reload production prediction service
      env:
        PRODUCTION_API_URL: ${{ secrets.PRODUCTION_API_URL }}
      run: |
        # Graceful model reload in production
        curl -X POST "${PRODUCTION_API_URL}/api/v1/admin/models/reload" \
          -H "Authorization: Bearer ${{ secrets.ADMIN_JWT_TOKEN }}" \
          -H "Content-Type: application/json" \
          -d '{"source": "production", "graceful": true}' || echo "Model reload endpoint not available yet"

        # Wait for model to load
        sleep 30

    - name: Validate production predictions
      env:
        PRODUCTION_API_URL: ${{ secrets.PRODUCTION_API_URL }}
      run: |
        # Test production prediction generation
        python << 'EOF'
import requests
import os
import time

api_url = os.environ.get('PRODUCTION_API_URL', 'https://api.f1-analytics.example.com')

# Test with retries
for attempt in range(3):
    try:
        response = requests.get(
            f"{api_url}/api/v1/predictions/next-race",
            headers={"Authorization": f"Bearer {os.environ.get('ADMIN_JWT_TOKEN', 'test-token')}"},
            timeout=30
        )

        if response.status_code == 200:
            predictions = response.json()
            print(f"‚úÖ Production predictions working")
            print(f"Model version: {predictions.get('model_version', 'unknown')}")

            # Validate prediction format
            pred_list = predictions.get('predictions', [])
            if pred_list:
                total_prob = sum(p.get('win_probability', 0) for p in pred_list)
                print(f"Total probability: {total_prob:.1f}%")

                if 95 <= total_prob <= 105:  # Allow some rounding tolerance
                    print("‚úÖ Probability normalization correct")
                else:
                    print(f"‚ö†Ô∏è Probability sum seems off: {total_prob}")

            break

        else:
            print(f"Attempt {attempt + 1}: API returned {response.status_code}")
            if attempt < 2:
                time.sleep(10)

    except requests.exceptions.RequestException as e:
        print(f"Attempt {attempt + 1}: Request failed: {e}")
        if attempt < 2:
            time.sleep(10)
else:
    print("‚ùå Could not validate production predictions")
    exit(1)
EOF

    - name: Update model performance monitoring
      run: |
        # Send deployment metrics to monitoring system
        curl -X POST "https://api.datadoghq.com/api/v1/events" \
          -H "DD-API-KEY: ${{ secrets.DATADOG_API_KEY }}" \
          -d '{
            "title": "ML Model Deployment - Production",
            "text": "New ML models deployed to production successfully",
            "tags": ["ml-deployment", "production", "f1-analytics", "models"],
            "alert_type": "info"
          }' || echo "Monitoring notification failed"

    - name: Notify deployment success
      if: success()
      uses: actions/github-script@v7
      with:
        script: |
          github.rest.repos.createCommitComment({
            owner: context.repo.owner,
            repo: context.repo.repo,
            commit_sha: context.sha,
            body: 'ü§ñ **ML Model Deployment Successful!**\n\n‚úÖ New Random Forest and XGBoost models deployed\n‚úÖ Production validation passed\n‚úÖ Prediction service updated\n\nüìä Ready to generate predictions for upcoming races'
          });

  # Monitor model performance
  monitor-performance:
    name: Monitor Model Performance
    runs-on: ubuntu-latest
    needs: deploy-production
    if: success()

    steps:
    - name: Set up monitoring alerts
      env:
        DATABASE_URL: ${{ secrets.PRODUCTION_DATABASE_URL }}
      run: |
        python << 'EOF'
import psycopg2
import os
from datetime import datetime, timedelta

# Connect to production database
conn = psycopg2.connect(os.environ['DATABASE_URL'])
cur = conn.cursor()

# Insert monitoring record
cur.execute("""
    INSERT INTO model_deployments (
        deployed_at,
        commit_sha,
        trigger_reason,
        status
    ) VALUES (%s, %s, %s, %s)
""", (
    datetime.utcnow(),
    os.environ.get('GITHUB_SHA', ''),
    os.environ.get('GITHUB_EVENT_NAME', 'unknown'),
    'deployed'
))

conn.commit()
conn.close()

print("‚úÖ Model deployment logged to database")
EOF

    - name: Schedule performance evaluation
      run: |
        # Set up a scheduled check for model performance after next race
        echo "Model performance will be evaluated after the next race completion"

        # In a real implementation, this might create a CloudWatch event
        # or add a job to a queue system to check prediction accuracy

  # Cleanup old models
  cleanup:
    name: Cleanup Old Models
    runs-on: ubuntu-latest
    needs: deploy-production
    if: success()

    steps:
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ secrets.AWS_REGION }}

    - name: Clean up old model versions
      run: |
        # Keep only the last 10 model versions
        aws s3 ls s3://${{ env.S3_BUCKET_MODELS }}/versions/ | sort -k1,2 | head -n -10 | while read line; do
          file=$(echo $line | awk '{print $4}')
          if [ -n "$file" ]; then
            echo "Deleting old model: $file"
            aws s3 rm s3://${{ env.S3_BUCKET_MODELS }}/versions/$file
          fi
        done

    - name: Archive old metrics
      run: |
        # Move metrics older than 90 days to archive
        CUTOFF_DATE=$(date -d '90 days ago' +%Y%m%d)

        aws s3 ls s3://${{ env.S3_BUCKET_MODELS }}/metrics/ | while read line; do
          file=$(echo $line | awk '{print $4}')
          file_date=$(echo $file | grep -o '[0-9]\{8\}' | head -1)

          if [ -n "$file_date" ] && [ "$file_date" -lt "$CUTOFF_DATE" ]; then
            echo "Archiving old metric file: $file"
            aws s3 mv s3://${{ env.S3_BUCKET_MODELS }}/metrics/$file s3://${{ env.S3_BUCKET_MODELS }}/archive/metrics/$file
          fi
        done